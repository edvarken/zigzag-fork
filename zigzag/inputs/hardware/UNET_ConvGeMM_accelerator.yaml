name: UNET_ConvGeMM_accelerator

memories:
  rf_1B_A: # 8bit Activations=Inputs loading regs
    size: 8
    r_bw: 8
    w_bw: 8
    r_cost: 0.01
    w_cost: 0.01
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    auto_cost_extraction: False
    operands: [I1]
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [] # Input regs fully unrolled over all MACs
    # but for Conv3x3, this is too much, since same 9 inputs are needed for every D1=K index
    # however we will need the unrolling of inputs over D1 for the GeMM where D1 = D or J, so keep it

  rf_2B_O: # 32 bit output=accumulation regs
    size: 32
    r_bw: 32
    w_bw: 32
    r_cost: 0.04
    w_cost: 0.04
    area: 0
    r_port: 2
    w_port: 2
    rw_port: 0
    latency: 1
    operands: [O]
    ports:
      - fh: w_port_1
        tl: r_port_1
        fl: w_port_2
        th: r_port_2
    served_dimensions: [] # accumulation registers fully unrolled over all MACs
    # but for Conv3x3, this is too much, since only 1 output for for every D3=(fy,fx) index
    # however we will need the unrolling of outputs over D3 for the GeMM where D3 = H, so keep it

  rf_1B_W: # 4bit Weights loading regs, however 4bit is only enough for Conv, while 8bit needed for Gemm :(
    size: 8
    r_bw: 8
    w_bw: 8
    r_cost: 0.01
    w_cost: 0.01
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    auto_cost_extraction: False
    operands: [I2] # I2 = weights
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [] # 1 weight reg. for each d1 col, for every d2 row and for each d3 index

  sram_2MB_A: # 2MB inputs/activations buffer
    size: 16777216
    r_bw: 64
    w_bw: 64
    r_cost: 416.16
    w_cost: 378.4
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    min_r_granularity: 64
    min_w_granularity: 64
    operands: [I1]
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [D1, D2, D3] # only 1 instance, that serves all D1, D2 dimensions

  sram_2MB_W: # 2MB weights buffer
    size: 16777216
    r_bw: 64
    w_bw: 64
    r_cost: 416.16
    w_cost: 378.4
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    min_r_granularity: 64
    min_w_granularity: 64
    operands: [I2]
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [D1, D2, D3] # only 1 instance, that serves all D1, D2 dimensions

  
  sram_4MB_O: # 4MB output buffer
    size: 33554432
    r_bw: 64
    w_bw: 64
    r_cost: 416.16
    w_cost: 378.4
    area: 0
    r_port: 0
    w_port: 0
    rw_port: 1
    latency: 1
    min_r_granularity: 64
    min_w_granularity: 64
    operands: [O]
    ports:
      - fh: rw_port_1 # this is about Outputs, so needs both (fh, tl) and (fl, th)
        tl: rw_port_1
        fl: rw_port_1
        th: rw_port_1
    served_dimensions: [D1, D2, D3]

  dram:
    size: 10000000000
    r_bw: 64
    w_bw: 64
    r_cost: 700
    w_cost: 750
    area: 0
    r_port: 0
    w_port: 0
    rw_port: 1
    latency: 1
    operands: [I1, I2, O]
    ports:
      - fh: rw_port_1
        tl: rw_port_1
      - fh: rw_port_1
        tl: rw_port_1
      - fh: rw_port_1 # this is about Outputs, so needs both (fh, tl) and (fl, th)
        tl: rw_port_1
        fl: rw_port_1
        th: rw_port_1
    served_dimensions: [D1, D2, D3] # this means there is only 1 instance of DRAM, and it serves all D1 and D2 dimensions

operational_array:
  unit_energy: 0.04 # pJ
  unit_area: 1 # unit
  dimensions: [D1, D2, D3]
  sizes: [32, 32, 9]

