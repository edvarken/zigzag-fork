name: UNET_GeMM_accelerator

memories:
  rf_1B_A: # 8bit Activations=Inputs loading regs
    size: 8
    r_bw: 8
    w_bw: 8
    r_cost: 0.01
    w_cost: 0.01
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    auto_cost_extraction: False
    operands: [I1]
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [] # Input regs fully unrolled over all MACs

  rf_2B_O: # 32 bit output=accumulation regs
    size: 32
    r_bw: 32
    w_bw: 32
    r_cost: 0.04
    w_cost: 0.04
    area: 0
    r_port: 2
    w_port: 2
    rw_port: 0
    latency: 1
    operands: [O]
    ports:
      - fh: w_port_1
        tl: r_port_1
        fl: w_port_2
        th: r_port_2
    served_dimensions: [] # accumulation registers fully unrolled over all MACs

  rf_1B_W: # 4bit Weights loading regs, however 4bit is only enough for Conv, while 8bit needed for Gemm :(
    size: 8
    r_bw: 8
    w_bw: 8
    r_cost: 0.01
    w_cost: 0.01
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    auto_cost_extraction: False
    operands: [I2]
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [D1] # 1 weight reg per row 

  sram_2MB_A: # 2MB inputs/activations buffer
    size: 16777216
    r_bw: 64
    w_bw: 64
    r_cost: 416.16
    w_cost: 378.4
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    min_r_granularity: 64
    min_w_granularity: 64
    operands: [I1]
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [D1, D2] # only 1 instance, that serves all D1, D2 dimensions

  sram_2MB_W: # 2MB weights buffer
    size: 16777216
    r_bw: 64
    w_bw: 64
    r_cost: 416.16
    w_cost: 378.4
    area: 0
    r_port: 1
    w_port: 1
    rw_port: 0
    latency: 1
    min_r_granularity: 64
    min_w_granularity: 64
    operands: [I2]
    ports:
      - fh: w_port_1
        tl: r_port_1
    served_dimensions: [D1, D2] # only 1 instance, that serves all D1, D2 dimensions

  
  sram_4MB_O: # 4MB output buffer
    size: 33554432
    r_bw: 64
    w_bw: 64
    r_cost: 416.16
    w_cost: 378.4
    area: 0
    r_port: 0
    w_port: 0
    rw_port: 1
    latency: 1
    min_r_granularity: 64
    min_w_granularity: 64
    operands: [O]
    ports:
      - fh: rw_port_1 # this is about Outputs, so needs both (fh, tl) and (fl, th)
        tl: rw_port_1
        fl: rw_port_1
        th: rw_port_1
    served_dimensions: [D1, D2]

  dram:
    size: 10000000000
    r_bw: 64
    w_bw: 64
    r_cost: 700
    w_cost: 750
    area: 0
    r_port: 0
    w_port: 0
    rw_port: 1
    latency: 1
    operands: [I1, I2, O]
    ports:
      - fh: rw_port_1
        tl: rw_port_1
      - fh: rw_port_1
        tl: rw_port_1
      - fh: rw_port_1 # this is about Outputs, so needs both (fh, tl) and (fl, th)
        tl: rw_port_1
        fl: rw_port_1
        th: rw_port_1
    served_dimensions: [D1, D2] # this means there is only 1 instance of DRAM, and it serves all D1 and D2 dimensions

operational_array:
  unit_energy: 0.04 # pJ
  unit_area: 1 # unit
  dimensions: [D1, D2]
  sizes: [32, 32]

