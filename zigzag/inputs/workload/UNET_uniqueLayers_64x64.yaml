- id: 0 # init conv, stride 1, padding=3 around all four sides, with a kernel of 7x7 this means (input height, input width) = (output height, output width)
  name: init_conv
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX] # This should not include dimensions defined in the dimension_relations.
  loop_sizes: [1, 64, 3, 64, 64, 7, 7] # (OY, OX) = (64, 64) same as input dimensions
  operand_precision:
    W: 8
    I: 8
    O: 16 # partial output precision
    O_final: 8 # final output precision: does it use rounding or truncation?
  operand_source: # The layer id the input operands of this layer originate from, should be set to the id of current layer if it doesnâ€™t originate from prior layers.
    I: 0
    W: 0

# downs0_ResnetBlock1: dim=64, dim_out=64
- id: 1 # padding=1 around all four sides, with a kernel of 3x3 this means (input height, input width) = (output height, output width)
  name: downs0_ResnetBlock1_block1_WeightStandardizedConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 64, 64, 64, 3, 3] # (OY, OX) = (64, 64) same as input dimensions
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 0
    W: 1

# downs0_ResnetBlock1_block2_SiLU_output + nn.Identity(x=input latent of downs0_ResnetBlock1):
- id: 2 # Residual Addition of layer 0 (init_conv) and layer 2 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 64, 64, 64] # G = 64 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 0
    I: 1

# downs0_linearattentionblock:
  # save current latent x for later residual connection
  # downs0_LayerNorm(dim_in) # dim_in = 64
  # LinearAttention(dim_in)): 
      # 2a. b, c, h, w = x.shape (b=1, c=64, h=64, w=64)
      # 2b. qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False).chunk(3, dim = 1): # hidden_dim=128, bias is TRUE by default, so here it is set to FALSE!, split in three 64x64 matrix CHUNKS each with hidden_dim channels
- id: 3
  name: downs0_linearattention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 64, 64, 64, 1, 1] # K = hidden_dim * 3
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 2
    W: 3
      # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv) # '1 (128=4*32) x=64 y=64 -> 1 4 32 (4096=x*y)'
      # 2d. compute SoftMax along channel c dimension
      # 2e. compute SoftMax along sequence length (x y)  dimension
      # 2f. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
      # 2g. v = v / (h * w) # scale the values by heigth*width
      # 2h. context = torch.einsum('b h d n, b h e n -> b h d e', k, v): # matrix matrix multiplication along 'n' dimension
- id: 4
  name: downs0_linearattention_contextGemm # k and v are both matrices with size: torch.Size([1, 4, 32, 4096])
  operator_type: Gemm
  equation: O[b][h][d][e]+=W[b][h][d][n]*I[b][h][e][n]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=key-matrix channels, E=value-matrix channels, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 4096]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 4 # I = values-matrix after chunking and rearranging of layer 3, so set to this layer's index instead..
    W: 4 # W = keys-matrix after chunking and rearranging of layer 3, so set to this layer's index instead..
      # 2i. out = torch.einsum('b h d e, b h d n -> b h e n', context, q) # matrix matrix multiplication along 'd' dimension
- id: 5
  name: downs0_linearattention_outGemm # context: torch.Size([1, 4, 32, 32]), while q: torch.Size([1, 4, 32, 4096])
  operator_type: Gemm
  equation: O[b][h][e][n]+=W[b][h][d][e]*I[b][h][d][n] # out has shape [1, 4, 32, 4096]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=32, E=32, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 4096]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 5 # I = queries-matrix after chunking and rearranging of layer 3, so set to this layer's index instead..
    W: 4 # W = context-matrix computed in previous layer
      # 2j. out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w) # out now has shape back: [1, 128, 64, 64]
      # 2k. to_out = nn.Sequential(nn.Conv2d(hidden_dim=dim_head * heads, dim, 1), LayerNorm(dim)) # hidden_dim=32*4=128, dim=64
- id: 6
  name: downs0_linearattention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 128, 64, 64, 1, 1]  #K=64 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 5 # previous layer = out
    W: 6
  # return x + to_out # residual connection:
- id: 7 # Residual Addition of layer 2 (input of downs0_linearattentionblock) and layer 6 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 64, 64, 64]
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 2
    I: 6

# downs0_Downsample(dim_in, dim_out): # with (dim_in, dim_out) = (64, 64)
  # 1. Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2) # [1, 64 * 4, 64 / 2, 64 / 2]
  # 2. nn.Conv2d(dim * 4, default(dim_out, dim), 1) # this is a 1x1 convolution to bring channels from dim*4 to dim_out if provided, else to dim.
- id: 8
  name: downs0_Downsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 256, 32, 32, 1, 1] # K=64 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 8 # previous layer 7 but after rearranging, so set to this layer's index instead..
    W: 8



# downs1_ResnetBlock1: dim=64, dim_out=64
- id: 9 # padding=1 around all four sides, with a kernel of 3x3 this means (input height, input width) = (output height, output width)
  name: downs1_ResnetBlock1_block1_WeightStandardizedConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 64, 32, 32, 3, 3] # (OY, OX) = (32, 32) same as input dimensions
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 8
    W: 9

# downs1_ResnetBlock1_block2_SiLU_output + nn.Identity(x=input latent of downs1_ResnetBlock1):
- id: 10 # Residual Addition of layer 8 (input latent of downs1_ResnetBlock1) and layer 9 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 64, 32, 32] # G = 64 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 8
    I: 9

# downs1_linearattentionblock:
  # save current latent x for later residual connection
  # downs1_LayerNorm(dim_in) # dim_in = 64
  # LinearAttention(dim_in)): 
      # 2a. b, c, h, w = x.shape (b=1, c=64, h=32, w=32)
      # 2b. qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False).chunk(3, dim = 1): # hidden_dim=128, bias is TRUE by default, so here it is set to FALSE!, split in three 32x32 matrix CHUNKS each with hidden_dim channels
- id: 11
  name: downs1_linearattention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 64, 32, 32, 1, 1] # K = hidden_dim * 3
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 10
    W: 11
      # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv) # '1 (128=4*32) x=32 y=32 -> 1 4 32 (1024=x*y)'
      # 2d. compute SoftMax along channel c dimension
      # 2e. compute SoftMax along sequence length (x y)  dimension
      # 2f. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
      # 2g. v = v / (h * w) # scale the values by heigth*width
      # 2h. context = torch.einsum('b h d n, b h e n -> b h d e', k, v): # matrix matrix multiplication along 'n' dimension
- id: 12
  name: downs1_linearattention_contextGemm # k and v are both matrices with size: torch.Size([1, 4, 32, 1024])
  operator_type: Gemm
  equation: O[b][h][d][e]+=W[b][h][d][n]*I[b][h][e][n]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=key-matrix channels, E=value-matrix channels, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 1024]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 12 # I = values-matrix after chunking and rearranging of layer 11, so set to this layer's index instead..
    W: 12 # W = keys-matrix after chunking and rearranging of layer 11, so set to this layer's index instead..
      # 2i. out = torch.einsum('b h d e, b h d n -> b h e n', context, q) # matrix matrix multiplication along 'd' dimension
- id: 13
  name: downs1_linearattention_outGemm # context: torch.Size([1, 4, 32, 32]), while q: torch.Size([1, 4, 32, 1024])
  operator_type: Gemm
  equation: O[b][h][e][n]+=W[b][h][d][e]*I[b][h][d][n] # out has shape [1, 4, 32, 1024]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=32, E=32, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 1024]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 13 # I = queries-matrix after chunking and rearranging of layer 11, so set to this layer's index instead..
    W: 12 # W = context-matrix computed in previous layer
      # 2j. out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w) # out now has shape back: [1, 128, 32, 32]
      # 2k. to_out = nn.Sequential(nn.Conv2d(hidden_dim=dim_head * heads, dim, 1), LayerNorm(dim)) # hidden_dim=32*4=128, dim=64
- id: 14
  name: downs1_linearattention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 128, 32, 32, 1, 1]  #K=64 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 13 # previous layer = out
    W: 14
  # return x + to_out # residual connection:
- id: 15 # Residual Addition of layer 10 (input of downs1_linearattentionblock) and layer 14 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 64, 32, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 10
    I: 14

# downs1_Downsample(dim_in, dim_out): # with (dim_in, dim_out) = (64, 128)
  # 1. Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2) # [1, 64 * 4, 32 / 2, 32 / 2]
  # 2. nn.Conv2d(dim * 4, default(dim_out, dim), 1) # this is a 1x1 convolution to bring channels from dim*4 to dim_out if provided, else to dim.
- id: 16
  name: downs1_Downsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 256, 16, 16, 1, 1] # K=128 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 16 # previous layer 15 but after rearranging, so set to this layer's index instead..
    W: 16


# downs2_ResnetBlock1: dim=128, dim_out=128
- id: 17 # padding=1 around all four sides, with a kernel of 3x3 this means (input height, input width) = (output height, output width)
  name: downs2_ResnetBlock1_block1_WeightStandardizedConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 128, 16, 16, 3, 3] # (OY, OX) = (16, 16) same as input dimensions
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 16
    W: 17

# downs2_ResnetBlock1_block2_SiLU_output + nn.Identity(x=input latent of downs2_ResnetBlock1):
- id: 18 # Residual Addition of layer 16 (input latent of downs2_ResnetBlock1) and layer 17 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 128, 16, 16] # G = 128 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 16
    I: 17

# downs2_attentionblock
  # save current latent x for later residual connection
  # downs2_LayerNorm(dim_in) # dim_in = 128
  # Attention(dim_in): (only 1 softmax instead of 2)
    # 2a. b, c, h, w = x.shape # (b=1, c=128, h=16, w=16)
    # 2b. qkv = self.to_qkv(x).chunk(3, dim=1) with to_qkv() = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False) # split in three 16x16 matrix CHUNKS each with hidden_dim channels
- id: 19
  name: downs2_attention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 128, 16, 16, 1, 1] # K = hidden_dim * 3 = 384, C = 128
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 18
    W: 19
    # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)
    # 2d. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
    # 2e. sim = einsum('b h d i, b h d j -> b h i j', q, k) # sim=similarity
- id: 20
  name: downs2_attention_simGemm # q and k are both matrices with size: torch.Size([1, 4, 32, 256])
  operator_type: Gemm
  equation: O[b][h][i][j]+=W[b][h][d][i]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=q-matrix sequence length, J=k-matrix sequence length, D=channels)
  loop_sizes: [1, 4, 256, 256, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 20 # I = k-matrix after chunking and rearranging of layer 18, so set to this layer's index instead..
    W: 20 # W = q-matrix after chunking and rearranging of layer 18, so set to this layer's index instead..   

    # 2f. attn = sim.softmax(dim=-1)
    # 2g. out = einsum('b h i j, b h d j -> b h i d', attn, v)
- id: 21
  name: downs2_attention_outGemm # attn is torch.Size([1, 4, 256, 256]) and v is torch.Size([1, 4, 32, 256])
  operator_type: Gemm
  equation: O[b][h][i][d]+=W[b][h][i][j]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=attn-matrix sequence length, J=v-matrix sequence length, D=channels)
  loop_sizes: [1, 4, 256, 256, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 21 # I = v-matrix after chunking and rearranging of layer 19, so set to this layer's index instead..
    W: 20 # W = attn-matrix after softmax of layer 20, so we can set to this previous layer 20.  
    # 2h. out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w) # out goes from [1, 4, 256, 32] to [1, 128, 16, 16]
    # 2i. self.to_out(out) with to_out() = nn.Conv2d(hidden_dim, dim, 1):
- id: 22
  name: downs2_attention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 128, 16, 16, 1, 1] # K=128 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 21 # previous layer = out
    W: 22
  # return x + to_out
- id: 23 # Residual Addition of layer 18 (input of downs2_attentionblock) and layer 22 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 128, 16, 16]
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 18
    I: 22

# downs2_Downsample(dim_in, dim_out): # with (dim_in, dim_out) = (128, 256)
  # 1. Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2) # [1, 128 * 4, 16 / 2, 16 / 2]
  # 2. nn.Conv2d(dim * 4, default(dim_out, dim), 1) # this is a 1x1 convolution to bring channels from dim*4=128*4=512 to dim_out=256 if provided, else to dim.
- id: 24
  name: downs2_Downsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 512, 8, 8, 1, 1] # K=256 output channels, and OY=OX=8
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 24 # previous layer 23 but after rearranging, so set to this layer's index instead..
    W: 24

# downs3_ResnetBlock1: dim=256, dim_out=256
- id: 25 # padding=1 around all four sides, with a kernel of 3x3 this means (input height, input width) = (output height, output width)
  name: downs3_ResnetBlock1_block1_WeightStandardizedConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 256, 8, 8, 3, 3] # (OY, OX) = (8, 8) same as input dimensions
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 24
    W: 25

# downs3_ResnetBlock1_block2_SiLU_output + nn.Identity(x=input latent of downs3_ResnetBlock1):
- id: 26 # Residual Addition of layer 24 (input latent of downs3_ResnetBlock1) and layer 25 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 256, 8, 8] # G = 256 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 24
    I: 25

# downs3_attentionblock=Residual(PreNorm(dim_in, Attention(dim_in))): # dim_in = 256    
  # save current latent x for later residual connection
  # downs3_LayerNorm(dim_in) # dim_in = 256
  # Attention(dim_in): (only 1 softmax instead of 2)
    # 2a. b, c, h, w = x.shape # (b=1, c=256, h=8, w=8)
    # 2b. qkv = self.to_qkv(x).chunk(3, dim=1) with to_qkv() = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False) # split in three 8x8 matrix CHUNKS each with hidden_dim channels
- id: 27
  name: downs3_attention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 256, 8, 8, 1, 1] # K = hidden_dim * 3 = 384, C = 256
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 26
    W: 27
    # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)
    # 2d. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
    # 2e. sim = einsum('b h d i, b h d j -> b h i j', q, k) # sim=similarity
- id: 28
  name: downs3_attention_simGemm # q and k are both matrices with size: torch.Size([1, 4, 32, 64])
  operator_type: Gemm
  equation: O[b][h][i][j]+=W[b][h][d][i]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=q-matrix sequence length, J=k-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 64, 64, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 28 # I = k-matrix after chunking and rearranging of layer 27, so set to this layer's index instead..
    W: 28 # W = q-matrix after chunking and rearranging of layer 27, so set to this layer's index instead..     
    # 2f. attn = sim.softmax(dim=-1)
    # 2g. out = einsum('b h i j, b h d j -> b h i d', attn, v)
- id: 29
  name: downs3_attention_outGemm # attn is torch.Size([1, 4, 64, 64]) and v is torch.Size([1, 4, 32, 64])
  operator_type: Gemm
  equation: O[b][h][i][d]+=W[b][h][i][j]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=attn-matrix sequence length, J=v-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 64, 64, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 29 # I = v-matrix after chunking and rearranging of layer 27, so set to this layer's index instead..
    W: 28 # W = attn-matrix after softmax of layer 28, so we can set to this previous layer 28.     
    # 2h. out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)
    # 2i. self.to_out(out) with to_out() = nn.Conv2d(hidden_dim, dim, 1) # hidden_dim=128, dim=256
- id: 30
  name: downs3_attention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 128, 8, 8, 1, 1] # K=256 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 29 # previous layer = out
    W: 30

  # return x + to_out
- id: 31 # Residual Addition of layer 26 (input of downs3_attentionblock) and layer 30 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 256, 8, 8] # G = 256 channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 26
    I: 30

# INSTEAD OF DOWNSAMPLE() JUST A FINAL 'DOWNSAMPLING' CONVOLUTION: WITHOUT DIVIDING HEIGHT/2 AND WIDTH/2, SO STAYS AT (8, 8) ! 
# nn.Conv2d(dim_in, dim_out, 3, padding=1) # with (dim_in, dim_out) = (256, 512)
- id: 32
  name: downs3_Downsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 256, 8, 8, 3, 3] # K=512 output channels!!, OY and OX stay at 8 !!
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 31
    W: 32

# mid_ResnetBlock1: dim=512, dim_out=512
- id: 33 # padding=1 around all four sides, with a kernel of 3x3 this means (input height, input width) = (output height, output width)
  name: mid_ResnetBlock1_block1_WeightStandardizedConv2d # nn.Conv2d(dim, dim_out, 3, padding = 1): # dim = dim_out = 512
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 512, 8, 8, 3, 3]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 32
    W: 33

- id: 34 # Residual Addition of layer 32 (input latent of mid_ResnetBlock1) and layer 33 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 512, 8, 8] # G = 512 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 32
    I: 33

# mid_attentionblock:
  # save current latent x for later residual connection
  # mid_LayerNorm(mid_dim)
  # Attention(mid_dim): only 1 softmax instead of 2
    # 2a. b, c, h, w = x.shape # (b=1, c=512, h=8, w=8)
    # 2b. qkv = self.to_qkv(x).chunk(3, dim=1) with to_qkv() = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False) # split in three 8x8 matrix CHUNKS each with hidden_dim channels
- id: 35
  name: mid_attention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 512, 8, 8, 1, 1] # K = hidden_dim * 3 = 384, C = 512
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 34
    W: 35
    # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)
    # 2d. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
    # 2e. sim = einsum('b h d i, b h d j -> b h i j', q, k)
- id: 36
  name: mid_attention_simGemm # q and k are both matrices with size: torch.Size([1, 4, 32, 64])
  operator_type: Gemm
  equation: O[b][h][i][j]+=W[b][h][d][i]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=q-matrix sequence length, J=k-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 64, 64, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 36 # I = k-matrix after chunking and rearranging of layer 35, so set to this layer's index instead..
    W: 36 # W = q-matrix after chunking and rearranging of layer 35, so set to this layer's index instead..     
    # 2f. attn = sim.softmax(dim=-1)
    # 2g. out = einsum('b h i j, b h d j -> b h i d', attn, v)
- id: 37
  name: mid_attention_outGemm # attn is torch.Size([1, 4, 64, 64]) and v is torch.Size([1, 4, 32, 64])
  operator_type: Gemm
  equation: O[b][h][i][d]+=W[b][h][i][j]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=attn-matrix sequence length, J=v-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 64, 64, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 37 # I = v-matrix after chunking and rearranging of layer 35, so set to this layer's index instead..
    W: 36 # W = attn-matrix after softmax of layer 36, so we can set to this previous layer 36.   
    # 2h. out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = height, y = width)
    # 2i. self.to_out(out) with to_out() = nn.Conv2d(hidden_dim, dim, 1) # hidden_dim=128, dim=512
- id: 38
  name: mid_attention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 128, 8, 8, 1, 1] # K=512 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 37 # previous layer = out
    W: 38

  # return x + to_out
- id: 39 # Residual Addition of layer 34 (input of mid_attentionblock) and layer 38 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 512, 8, 8] # G = 512 channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 34
    I: 38


# mid_ResnetBlock2: dim=512, dim_out=512
- id: 40 # padding=1 around all four sides, with a kernel of 3x3 this means (input height, input width) = (output height, output width)
  name: mid_ResnetBlock2_block1_WeightStandardizedConv2d # nn.Conv2d(dim, dim_out, 3, padding = 1): # dim = dim_out = 512
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 512, 8, 8, 3, 3]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 39
    W: 40

- id: 41 # Residual Addition of layer 56 (input latent of mid_ResnetBlock2) and layer 40 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 512, 8, 8] # G = 512 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 39
    I: 40

# in the following self.ups block_klasses, the in/out dimensions are not the same, so each ResNetblock will have an additional res_conv happening after block2, before the residual connection.
# (dim_out+dim_in, dim_out)=[(512+256, 512), (256+128, 256), (128+64, 128), (64+64, 64)]

# ups0_ResnetBlock1: (512 channels from main path) + (output of downs3_attentionblock with 256 channels), dim_out=512 # this is a long skip connection: x = torch.cat((x, h.pop()), dim=1), NOT AN ADDITION but a CONCATENATION
  # ups0_ResnetBlock1_block1_WeightStandardizedConv2d
- id: 42
  name: ups0_ResnetBlock1_block1_WeightStandardizedConv2d # nn.Conv2d(dim_out+dim_in, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 768, 8, 8, 3, 3] # 3x3 kernel
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 41
    W: 42
  # ups0_ResnetBlock1_block2_WeightStandardizedConv2d
- id: 43
  name: ups0_ResnetBlock1_block2_WeightStandardizedConv2d # nn.Conv2d(dim_out, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 512, 8, 8, 3, 3]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 40
    W: 43
  # ups0_ResnetBlock1_block2_SiLU_output + Conv2d(dim, dim_out, 1)(layer 41= input_latent of ups0_ResnetBlock1) # with dim=768 and dim_out=512
- id: 44
  name: ups0_ResnetBlock1_block2_residual_conv # nn.Conv2d(dim_out+dim_in, dim_out, 1, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 768, 8, 8, 1, 1] # a 1x1 kernel instead of 3x3!
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 41
    W: 44
- id: 45 # Residual Addition of layer 44 (residual_conv of input of ups0_ResnetBlock1) and layer 43 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 512, 8, 8] # G = 512 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 44
    I: 43

# ups0_attentionblock: # dim_out = 512
  # save current latent x for later residual connection
  # ups0_LayerNorm(dim_out)
  # Attention(dim_out): only 1 softmax instead of 2
    # 2a. b, c, h, w = x.shape # (b=1, c=512, h=8, w=8)
    # 2b. qkv = self.to_qkv(x).chunk(3, dim=1) with to_qkv() = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False) # split in three 8x8 matrix CHUNKS each with hidden_dim channels
- id: 46
  name: ups0_attention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 512, 8, 8, 1, 1] # K = hidden_dim * 3 = 384, C = 512
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 45
    W: 46
    # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)
    # 2d. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
    # 2e. sim = einsum('b h d i, b h d j -> b h i j', q, k)
- id: 47
  name: ups0_attention_simGemm # q and k are both matrices with size: torch.Size([1, 4, 32, 64])
  operator_type: Gemm
  equation: O[b][h][i][j]+=W[b][h][d][i]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=q-matrix sequence length, J=k-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 64, 64, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 47 # I = k-matrix after chunking and rearranging of layer 46, so set to this layer's index instead..
    W: 47 # W = q-matrix after chunking and rearranging of layer 46, so set to this layer's index instead..     
    # 2f. attn = sim.softmax(dim=-1)
    # 2g. out = einsum('b h i j, b h d j -> b h i d', attn, v)
- id: 48
  name: ups0_attention_outGemm # attn is torch.Size([1, 4, 64, 64]) and v is torch.Size([1, 4, 32, 64])
  operator_type: Gemm
  equation: O[b][h][i][d]+=W[b][h][i][j]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=attn-matrix sequence length, J=v-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 64, 64, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 48 # I = v-matrix after chunking and rearranging of layer 46, so set to this layer's index instead..
    W: 47 # W = attn-matrix after softmax of layer 47, so we can set to this previous layer 47.   
    # 2h. out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)
    # 2i. self.to_out(out) with to_out() = nn.Conv2d(hidden_dim, dim, 1) # hidden_dim=128, dim=512
- id: 49
  name: ups0_attention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 512, 128, 8, 8, 1, 1] # K=512 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 48 # previous layer = out
    W: 49
  # return x + to_out
- id: 50 # Residual Addition of layer 45 (input of ups0_attentionblock) and layer 49 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 512, 8, 8] # G = 512 channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 45
    I: 49
# ups0_Upsample(dim_out, dim_in): dim_out=512, dim_in=256
  # 1. nn.Upsample(scale_factor=2, mode='nearest') # LATENT becomes (batch_size=1, channels=512, heigh=16, width=16)
  # 2. nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1) # with (dim, dim_out) = (512, 256) # dim_out becomes = dim, while dim_in becomes = dim_out
- id: 51
  name: ups0_upsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 512, 16, 16, 3, 3] # latent becomes torch.Size([1, 256, 16, 16])
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 50
    W: 51


# ups1_ResnetBlock1: (256 channels from main path) + (output of downs2_attentionblock with 128 channels), dim_out=256 # this is a long skip connection: x = torch.cat((x, h.pop()), dim=1), NOT AN ADDITION but a CONCATENATION
  # ups1_ResnetBlock1_block1_WeightStandardizedConv2d
- id: 52
  name: ups1_ResnetBlock1_block1_WeightStandardizedConv2d # nn.Conv2d(dim_out+dim_in, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 384, 16, 16, 3, 3] # 3x3 kernel
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 51
    W: 52
  # ups1_ResnetBlock1_block2_WeightStandardizedConv2d
- id: 53
  name: ups1_ResnetBlock1_block2_WeightStandardizedConv2d # nn.Conv2d(dim_out, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 256, 16, 16, 3, 3]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 52
    W: 53
  # ups1_ResnetBlock1_block2_SiLU_output + Conv2d(dim, dim_out, 1)(layer 51= input_latent of ups1_ResnetBlock1) # with dim=384 and dim_out=256
- id: 54
  name: ups1_ResnetBlock1_block2_residual_conv # nn.Conv2d(dim_out+dim_in, dim_out, 1, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 384, 16, 16, 1, 1] # a 1x1 kernel instead of 3x3!
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 51
    W: 54
- id: 55 # Residual Addition of layer 54 (residual_conv of input of ups1_ResnetBlock1) and layer 53 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 256, 16, 16] # G = 256 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 54
    I: 53

# ups1_attentionblock: # dim_out = 256
  # save current latent x for later residual connection
  # ups1_LayerNorm(dim_out)
  # Attention(dim_out): only 1 softmax instead of 2
    # 2a. b, c, h, w = x.shape # (b=1, c=256, h=16, w=16)
    # 2b. qkv = self.to_qkv(x).chunk(3, dim=1) with to_qkv() = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False) # split in three 16x16 matrix CHUNKS each with hidden_dim channels
- id: 56
  name: ups1_attention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 256, 16, 16, 1, 1] # K = hidden_dim * 3 = 384, C = 256
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 55
    W: 56
    # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)
    # 2d. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
    # 2e. sim = einsum('b h d i, b h d j -> b h i j', q, k)
- id: 57
  name: ups1_attention_simGemm # q and k are both matrices with size: torch.Size([1, 4, 32, 256])
  operator_type: Gemm
  equation: O[b][h][i][j]+=W[b][h][d][i]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=q-matrix sequence length, J=k-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 256, 256, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 57 # I = k-matrix after chunking and rearranging of layer 56, so set to this layer's index instead..
    W: 57 # W = q-matrix after chunking and rearranging of layer 56, so set to this layer's index instead..     
    # 2f. attn = sim.softmax(dim=-1)
    # 2g. out = einsum('b h i j, b h d j -> b h i d', attn, v)
- id: 58
  name: ups1_attention_outGemm # attn is torch.Size([1, 4, 256, 256]) and v is torch.Size([1, 4, 32, 256])
  operator_type: Gemm
  equation: O[b][h][i][d]+=W[b][h][i][j]*I[b][h][d][j]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, I, J, D] # (B=batch_size, H=heads, I=attn-matrix sequence length, J=v-matrix sequence length, D=channels=32 always)
  loop_sizes: [1, 4, 256, 256, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 58 # I = v-matrix after chunking and rearranging of layer 56, so set to this layer's index instead..
    W: 57 # W = attn-matrix after softmax of layer 57, so we can set to this previous layer 57.   
    # 2h. out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)
    # 2i. self.to_out(out) with to_out() = nn.Conv2d(hidden_dim, dim, 1) # hidden_dim=128, dim=256
- id: 59
  name: ups1_attention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 256, 128, 16, 16, 1, 1] # K=256 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 58 # previous layer = out
    W: 59
  # return x + to_out
- id: 60 # Residual Addition of layer 55 (input of ups1_attentionblock) and layer 59 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 256, 16, 16] # G = 256 channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 55
    I: 59

# ups1_Upsample(dim_out, dim_in): dim_out=256, dim_in=128
  # 1. nn.Upsample(scale_factor=2, mode='nearest') # LATENT becomes (batch_size=1, channels=256, heigh=32, width=32)
  # 2. nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1) # with (dim, dim_out) = (256, 128) # dim_out becomes = dim, while dim_in becomes = dim_out
- id: 61
  name: ups1_upsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 256, 32, 32, 3, 3] # latent becomes torch.Size([1, 128, 32, 32])
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 60
    W: 61


# ups2_ResnetBlock1: (128 channels from main path) + (output of downs1_attentionblock with 64 channels), dim_out=128 # this is a long skip connection: x = torch.cat((x, h.pop()), dim=1), NOT AN ADDITION but a CONCATENATION
  # ups2_ResnetBlock1_block1_WeightStandardizedConv2d
- id: 62
  name: ups2_ResnetBlock1_block1_WeightStandardizedConv2d # nn.Conv2d(dim_out+dim_in, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 192, 32, 32, 3, 3] # 3x3 kernel
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 61
    W: 62
  # ups2_ResnetBlock1_block2_WeightStandardizedConv2d
- id: 63
  name: ups2_ResnetBlock1_block2_WeightStandardizedConv2d # nn.Conv2d(dim_out, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 128, 32, 32, 3, 3]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 62
    W: 63
  # ups2_ResnetBlock1_block2_SiLU_output + Conv2d(dim, dim_out, 1)(layer 61= input_latent of ups2_ResnetBlock1) # with dim=192 and dim_out=128
- id: 64
  name: ups2_ResnetBlock1_block2_residual_conv # nn.Conv2d(dim_out+dim_in, dim_out, 1, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 192, 32, 32, 1, 1] # a 1x1 kernel instead of 3x3!
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 61
    W: 64
- id: 65 # Residual Addition of layer 64 (residual_conv of input of ups2_ResnetBlock1) and layer 63 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 128, 32, 32] # G = 128 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 64
    I: 63


# ups2_linearattentionblock:
  # save current latent x for later residual connection
  # ups2_LayerNorm(dim_out) # dim_out = 128
  # LinearAttention(dim_out)): 
    # 2a. b, c, h, w = x.shape (b=1, c=128, h=32, w=32)
    # 2b. qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False).chunk(3, dim = 1): # hidden_dim=128, bias is TRUE by default, so here it is set to FALSE!, split in three 32x32 matrix CHUNKS each with hidden_dim channels
- id: 66
  name: ups2_linearattention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 128, 32, 32, 1, 1] # K = hidden_dim * 3
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 65
    W: 66
      # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv) # '1 (128=4*32) x=32 y=32 -> 1 4 32 (1024=x*y)'
      # 2d. compute SoftMax along channel c dimension
      # 2e. compute SoftMax along sequence length (x y)  dimension
      # 2f. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
      # 2g. v = v / (h * w) # scale the values by heigth*width
      # 2h. context = torch.einsum('b h d n, b h e n -> b h d e', k, v): # matrix matrix multiplication along 'n' dimension
- id: 67
  name: ups2_linearattention_contextGemm # k and v are both matrices with size: torch.Size([1, 4, 32, 1024])
  operator_type: Gemm
  equation: O[b][h][d][e]+=W[b][h][d][n]*I[b][h][e][n]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=key-matrix channels, E=value-matrix channels, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 1024]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 67 # I = values-matrix after chunking and rearranging of layer 66, so set to this layer's index instead..
    W: 67 # W = keys-matrix after chunking and rearranging of layer 66, so set to this layer's index instead..
      # 2i. out = torch.einsum('b h d e, b h d n -> b h e n', context, q) # matrix matrix multiplication along 'd' dimension
- id: 68
  name: ups2_linearattention_outGemm # context: torch.Size([1, 4, 32, 32]), while q: torch.Size([1, 4, 32, 1024])
  operator_type: Gemm
  equation: O[b][h][e][n]+=W[b][h][d][e]*I[b][h][d][n] # out has shape [1, 4, 32, 1024]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=32, E=32, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 1024]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 68 # I = queries-matrix after chunking and rearranging of layer 66, so set to this layer's index instead..
    W: 67 # W = context-matrix computed in previous layer
      # 2j. out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w) # out now has shape back: [1, 128, 32, 32]
      # 2k. to_out = nn.Sequential(nn.Conv2d(hidden_dim=dim_head * heads, dim, 1), LayerNorm(dim)) # hidden_dim=32*4=128, dim=128
- id: 69
  name: ups2_linearattention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 128, 128, 32, 32, 1, 1]  #K=128 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 68 # previous layer = out
    W: 69
  # return x + to_out # residual connection:
- id: 70 # Residual Addition of layer 65 (input of ups2_linearattentionblock) and layer 69 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 128, 32, 32]
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 65
    I: 69

# ups2_Upsample(dim_out, dim_in): dim_out=128, dim_in=64 
  # 1. nn.Upsample(scale_factor=2, mode='nearest') # LATENT becomes (batch_size=1, channels=128, heigh=64, width=64)
  # 2. nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1) # with (dim, dim_out) = (128, 64) # dim_out becomes = dim, while dim_in becomes = dim_out
- id: 71
  name: ups2_upsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 128, 64, 64, 3, 3] # latent becomes torch.Size([1, 64, 64, 64])
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 70
    W: 71



# ups3_ResnetBlock1: (64 channels from main path) + (output of downs0_attentionblock with 64 channels), dim_out=64 # this is a long skip connection: x = torch.cat((x, h.pop()), dim=1), NOT AN ADDITION but a CONCATENATION
  # ups3_ResnetBlock1_block1_WeightStandardizedConv2d:
- id: 72
  name: ups3_ResnetBlock1_block1_WeightStandardizedConv2d # nn.Conv2d(dim_out+dim_in, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 128, 64, 64, 3, 3] # 3x3 kernel
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 71
    W: 72
  # ups3_ResnetBlock1_block2_WeightStandardizedConv2d:
- id: 73
  name: ups3_ResnetBlock1_block2_WeightStandardizedConv2d # nn.Conv2d(dim_out, dim_out, 3, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 64, 64, 64, 3, 3]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 72
    W: 73
  # ups3_ResnetBlock1_block2_SiLU_output + Conv2d(dim, dim_out, 1)(layer 71= input_latent of ups3_ResnetBlock1) # with dim=128 and dim_out=64
- id: 74
  name: ups3_ResnetBlock1_block2_residual_conv # nn.Conv2d(dim_out+dim_in, dim_out, 1, padding = 1):
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 128, 64, 64, 1, 1] # a 1x1 kernel instead of 3x3!
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 71
    W: 74
- id: 75 # Residual Addition of layer 74 (residual_conv of input of ups3_ResnetBlock1) and layer 73 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 64, 64, 64] # G = 64 = channels
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 74
    I: 73

# ups3_linearattentionblock:
  # save current latent x for later residual connection
  # ups2_LayerNorm(dim_out) # dim_out = 64
  # LinearAttention(dim_out)): 
    # 2a. b, c, h, w = x.shape (b=1, c=64, h=64, w=64)
    # 2b. qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False).chunk(3, dim = 1): # hidden_dim=128, bias is TRUE by default, so here it is set to FALSE!, split in three 64x64 matrix CHUNKS each with hidden_dim channels
- id: 76
  name: ups3_linearattention_qkvConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 384, 64, 64, 64, 1, 1] # K = hidden_dim * 3
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 75
    W: 76
      # 2c. q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv) # '1 (128=4*32) x=64 y=64 -> 1 4 32 (4096=x*y)'
      # 2d. compute SoftMax along channel c dimension
      # 2e. compute SoftMax along sequence length (x y)  dimension
      # 2f. q = q * self.scale # scale the queries by 1/sqrt(dim_head)
      # 2g. v = v / (h * w) # scale the values by heigth*width
      # 2h. context = torch.einsum('b h d n, b h e n -> b h d e', k, v): # matrix matrix multiplication along 'n' dimension
- id: 77
  name: ups3_linearattention_contextGemm # k and v are both matrices with size: torch.Size([1, 4, 32, 4096])
  operator_type: Gemm
  equation: O[b][h][d][e]+=W[b][h][d][n]*I[b][h][e][n]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=key-matrix channels, E=value-matrix channels, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 4096]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 77 # I = values-matrix after chunking and rearranging of layer 76, so set to this layer's index instead..
    W: 77 # W = keys-matrix after chunking and rearranging of layer 76, so set to this layer's index instead..
      # 2i. out = torch.einsum('b h d e, b h d n -> b h e n', context, q) # matrix matrix multiplication along 'd' dimension
- id: 78
  name: ups3_linearattention_outGemm # context: torch.Size([1, 4, 32, 32]), while q: torch.Size([1, 4, 32, 4096])
  operator_type: Gemm
  equation: O[b][h][e][n]+=W[b][h][d][e]*I[b][h][d][n] # out has shape [1, 4, 32, 4096]
  # dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, H, D, E, N] # (B=batch_size, H=heads, D=32, E=32, N=sequence length)
  loop_sizes: [1, 4, 32, 32, 4096]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 78 # I = queries-matrix after chunking and rearranging of layer 110, so set to this layer's index instead..
    W: 77 # W = context-matrix computed in previous layer
      # 2j. out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w) # out now has shape back: [1, 128, 64, 64]
      # 2k. to_out = nn.Sequential(nn.Conv2d(hidden_dim=dim_head * heads, dim, 1), LayerNorm(dim)) # hidden_dim=32*4=128, dim=64
- id: 79
  name: ups3_linearattention_to_outConv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 128, 64, 64, 1, 1]  #K=64 output channels
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 79 # previous layer = out
    W: 79
  # return x + to_out # residual connection:
- id: 80 # Residual Addition of layer 75 (input of ups3_linearattentionblock) and layer 79 (main path)
  operator_type: Add
  equation: O[b][g][oy][ox]=W[b][g][oy][ox]+I[b][g][oy][ox]
  loop_dims: [B, G, OY, OX]
  loop_sizes: [1, 64, 64, 64]
  operand_precision:
    W: 8
    I: 8
    O: 16
    O_final: 8
  operand_source:
    W: 75
    I: 79

# INSTEAD OF UPSAMPLE() JUST A FINAL 'UPSAMPLING' CONVOLUTION: 
# nn.Conv2d(dim_out, dim_in, 3, padding=1) # with (dim_out, dim_in) = (64, 64)
- id: 81
  name: ups3_upsample_Conv2d
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 64, 64, 64, 64, 3, 3] # latent becomes torch.Size([1, 64, 64, 64])
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 80
    W: 81
# I omitted the final resNetBLock here since reuses dimensions of downs0 and ups3_linearattention
# final_conv = nn.Conv2d(dim, out_dim, 1) # final convolution with dim=64 and out_dim = output channels = latents.shape[1] = 3
- id: 82
  name: final_conv
  operator_type: Conv
  equation: O[b][k][oy][ox]+=W[k][c][fy][fx]*I[b][c][iy][ix]
  dimension_relations: [ix=1*ox+1*fx, iy=1*oy+1*fy]
  loop_dims: [B, K, C, OY, OX, FY, FX]
  loop_sizes: [1, 3, 64, 64, 64, 1, 1] # a 1x1 kernel # K=3 output channels!
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 82
    W: 82