- id: 0
  name: encoder0_attention_QxK # Q and K are both matrices with size: [8, 4096, 40]
  operator_type: Gemm
  equation: O[h][l][j]+=W[h][l][d]*I[h][l][d] # O has size [8, 4096, 4096]
  loop_dims: [H, L, D, J] # (H=n_heads=8, L=sequence length=64*64, D=d_head= 40 = 320/8, J=L=4096)
  loop_sizes: [8, 4096, 40, 4096]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 0 # I = transposed K-matrix
    W: 0 # W = Q-matrix
- id: 1
  name: encoder0_attention_WEIGHTxV # WEIGHT has size [8,4096,4096] and V has size [8, 4096, 40]
  operator_type: Gemm
  equation: O[h][l][d]+=W[h][l][j]*I[h][l][d] # O has size [8, 4096, 40]
  loop_dims: [H, L, D, J]
  loop_sizes: [8, 4096, 40, 4096]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 1 # I = V-matrix
    W: 1 # W = WEIGHT-matrix

- id: 2
  name: mid_attention_QxK # Q and K are both matrices with size: [8, 64, 160]
  operator_type: Gemm
  equation: O[h][l][j]+=W[h][l][d]*I[h][l][d] # O has size [8, 64, 64]
  loop_dims: [H, L, D, J] # (H=n_heads=8, L=sequence length=8*8, D=d_head= 160 = 1280/8, J=64)
  loop_sizes: [8, 64, 160, 64]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 2 # I = transposed K-matrix
    W: 2 # W = Q-matrix
- id: 3
  name: mid_attention_WEIGHTxV # WEIGHT has size [8,64,64] and V has size [8, 64, 160]
  operator_type: Gemm
  equation: O[h][l][d]+=W[h][l][j]*I[h][l][d] # O has size [8, 64, 160]
  loop_dims: [H, L, D, J]
  loop_sizes: [8, 64, 160, 64]
  operand_precision:
    W: 8
    I: 8
    O: 16 
    O_final: 8 
  operand_source:
    I: 3 # I = V-matrix
    W: 3 # W = WEIGHT-matrix